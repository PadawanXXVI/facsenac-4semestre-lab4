{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0cadb3",
   "metadata": {},
   "source": [
    "# üê±üê∂ Classifica√ß√£o de Imagens: Cats vs Dogs (com TensorFlow Datasets)\n",
    "Este notebook foi adaptado para rodar no **VS Code + Windows 11 + Python 3.13.17**.\n",
    "Usaremos o **TensorFlow Datasets (TFDS)** para baixar automaticamente o dataset Cats vs Dogs.\n",
    "Cada c√©lula possui explica√ß√µes did√°ticas em Markdown e coment√°rios no c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa25015d",
   "metadata": {},
   "source": [
    "## üîπ 1. Prepara√ß√£o do Ambiente\n",
    "Execute este comando no terminal Bash do VS Code para instalar as depend√™ncias:\n",
    "```bash\n",
    "pip install tensorflow tensorflow-datasets matplotlib numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558694e4",
   "metadata": {},
   "source": [
    "## üîπ 2. Importa√ß√£o das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c985969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c7bdaa",
   "metadata": {},
   "source": [
    "## üîπ 3. Carregar Dataset (TFDS baixa automaticamente)\n",
    "- O dataset Cats vs Dogs √© baixado e armazenado localmente pelo TFDS.\n",
    "- Usamos `as_supervised=True` para retornar tuplas (imagem, r√≥tulo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5033e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder data\\raw\\cats_vs_dogs\\4.0.1 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to data\\raw\\cats_vs_dogs\\4.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 786/786 [00:41<00:00, 18.78 MiB/s]\n",
      "Dl Completed...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:41<00:00, 41.86s/ url]\n",
      "                                                                \r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"There is no item named 'PetImages\\\\\\\\Cat\\\\\\\\0.jpg' in the archive\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m data_dir = \u001b[33m\"\u001b[39m\u001b[33mdata/raw\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m os.makedirs(data_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m (ds_train, ds_test), ds_info = \u001b[43mtfds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcats_vs_dogs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain[:80\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain[80\u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m:]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Dataset carregado com sucesso!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\QuartoSemestre\\laboratorio-IV\\reconhecimento_imagens\\.venv\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:176\u001b[39m, in \u001b[36m_FunctionDecorator.__call__\u001b[39m\u001b[34m(self, function, instance, args, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m._start_call()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    178\u001b[39m   metadata.mark_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\QuartoSemestre\\laboratorio-IV\\reconhecimento_imagens\\.venv\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:666\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs, file_format)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[32m    542\u001b[39m \n\u001b[32m    543\u001b[39m \u001b[33;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    658\u001b[39m \u001b[33;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# fmt: skip\u001b[39;00m\n\u001b[32m    660\u001b[39m dbuilder = _fetch_builder(\n\u001b[32m    661\u001b[39m     name=name,\n\u001b[32m    662\u001b[39m     data_dir=data_dir,\n\u001b[32m    663\u001b[39m     builder_kwargs=builder_kwargs,\n\u001b[32m    664\u001b[39m     try_gcs=try_gcs,\n\u001b[32m    665\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    669\u001b[39m   as_dataset_kwargs = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\QuartoSemestre\\laboratorio-IV\\reconhecimento_imagens\\.venv\\Lib\\site-packages\\tensorflow_datasets\\core\\load.py:518\u001b[39m, in \u001b[36m_download_and_prepare_builder\u001b[39m\u001b[34m(dbuilder, download, download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m    517\u001b[39m   download_and_prepare_kwargs = download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m   \u001b[43mdbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\QuartoSemestre\\laboratorio-IV\\reconhecimento_imagens\\.venv\\Lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:176\u001b[39m, in \u001b[36m_FunctionDecorator.__call__\u001b[39m\u001b[34m(self, function, instance, args, kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m._start_call()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    178\u001b[39m   metadata.mark_error()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\QuartoSemestre\\laboratorio-IV\\reconhecimento_imagens\\.venv\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:763\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, download_dir, download_config, file_format, permissions)\u001b[39m\n\u001b[32m    761\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.read_from_directory(\u001b[38;5;28mself\u001b[39m.data_dir)\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[32m    769\u001b[39m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[32m    770\u001b[39m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[32m    771\u001b[39m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[32m    772\u001b[39m   \u001b[38;5;28mself\u001b[39m.info.download_size = dl_manager.downloaded_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\QuartoSemestre\\laboratorio-IV\\reconhecimento_imagens\\.venv\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1808\u001b[39m, in \u001b[36mGeneratorBasedBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download_config.max_examples_per_split == \u001b[32m0\u001b[39m:\n\u001b[32m   1806\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m split_infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[38;5;66;03m# Update the info object with the splits.\u001b[39;00m\n\u001b[32m   1811\u001b[39m split_dict = splits_lib.SplitDict(split_infos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\QuartoSemestre\\laboratorio-IV\\reconhecimento_imagens\\.venv\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1782\u001b[39m, in \u001b[36mGeneratorBasedBuilder._generate_splits\u001b[39m\u001b[34m(self, dl_manager, download_config)\u001b[39m\n\u001b[32m   1775\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m split_name, generator \u001b[38;5;129;01min\u001b[39;00m utils.tqdm(\n\u001b[32m   1776\u001b[39m       split_generators.items(),\n\u001b[32m   1777\u001b[39m       desc=\u001b[33m\"\u001b[39m\u001b[33mGenerating splits...\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1778\u001b[39m       unit=\u001b[33m\"\u001b[39m\u001b[33m splits\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1779\u001b[39m       leave=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1780\u001b[39m   ):\n\u001b[32m   1781\u001b[39m     filename_template = \u001b[38;5;28mself\u001b[39m._get_filename_template(split_name=split_name)\n\u001b[32m-> \u001b[39m\u001b[32m1782\u001b[39m     future = \u001b[43msplit_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit_split_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnondeterministic_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnondeterministic_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m     split_info_futures.append(future)\n\u001b[32m   1791\u001b[39m \u001b[38;5;66;03m# Process the result of the beam pipeline.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\QuartoSemestre\\laboratorio-IV\\reconhecimento_imagens\\.venv\\Lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:447\u001b[39m, in \u001b[36mSplitBuilder.submit_split_generation\u001b[39m\u001b[34m(self, split_name, generator, filename_template, disable_shuffling, nondeterministic_order)\u001b[39m\n\u001b[32m    442\u001b[39m     logging.warning(\n\u001b[32m    443\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m`nondeterministic_order` is set to True for a dataset that does\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    444\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m not use beam. Setting `disable_shuffling` to True.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    445\u001b[39m     )\n\u001b[32m    446\u001b[39m     build_kwargs[\u001b[33m'\u001b[39m\u001b[33mdisable_shuffling\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_from_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Otherwise, beam required\u001b[39;00m\n\u001b[32m    449\u001b[39m   unknown_generator_type = \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    450\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInvalid split generator value for split `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    451\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mExpected generator or apache_beam object. Got: \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    452\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(generator)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    453\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\QuartoSemestre\\laboratorio-IV\\reconhecimento_imagens\\.venv\\Lib\\site-packages\\tensorflow_datasets\\core\\split_builder.py:508\u001b[39m, in \u001b[36mSplitBuilder._build_from_generator\u001b[39m\u001b[34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[39m\n\u001b[32m    498\u001b[39m serialized_info = \u001b[38;5;28mself\u001b[39m._features.get_serialized_info()\n\u001b[32m    499\u001b[39m writer = writer_lib.Writer(\n\u001b[32m    500\u001b[39m     serializer=example_serializer.ExampleSerializer(serialized_info),\n\u001b[32m    501\u001b[39m     filename_template=filename_template,\n\u001b[32m   (...)\u001b[39m\u001b[32m    506\u001b[39m     ignore_duplicates=\u001b[38;5;28mself\u001b[39m._ignore_duplicates,\n\u001b[32m    507\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGenerating \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m examples...\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m examples\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_num_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmininterval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\QuartoSemestre\\laboratorio-IV\\reconhecimento_imagens\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\QuartoSemestre\\laboratorio-IV\\reconhecimento_imagens\\.venv\\Lib\\site-packages\\tensorflow_datasets\\image_classification\\cats_vs_dogs.py:119\u001b[39m, in \u001b[36mCatsVsDogs._generate_examples\u001b[39m\u001b[34m(self, archive)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m zipfile.ZipFile(buffer, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m new_zip:\n\u001b[32m    118\u001b[39m   new_zip.writestr(norm_fname, img_recoded.numpy())\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m new_fobj = \u001b[43mzipfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m record = {\n\u001b[32m    122\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m: new_fobj,\n\u001b[32m    123\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage/filename\u001b[39m\u001b[33m\"\u001b[39m: norm_fname,\n\u001b[32m    124\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: label,\n\u001b[32m    125\u001b[39m }\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m norm_fname, record\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python313\\Lib\\zipfile\\__init__.py:1639\u001b[39m, in \u001b[36mZipFile.open\u001b[39m\u001b[34m(self, name, mode, pwd, force_zip64)\u001b[39m\n\u001b[32m   1636\u001b[39m     zinfo.compress_level = \u001b[38;5;28mself\u001b[39m.compresslevel\n\u001b[32m   1637\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1638\u001b[39m     \u001b[38;5;66;03m# Get info object for name\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1639\u001b[39m     zinfo = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgetinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python313\\Lib\\zipfile\\__init__.py:1567\u001b[39m, in \u001b[36mZipFile.getinfo\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1565\u001b[39m info = \u001b[38;5;28mself\u001b[39m.NameToInfo.get(name)\n\u001b[32m   1566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1567\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   1568\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mThere is no item named \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m in the archive\u001b[39m\u001b[33m'\u001b[39m % name)\n\u001b[32m   1570\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "\u001b[31mKeyError\u001b[39m: \"There is no item named 'PetImages\\\\\\\\Cat\\\\\\\\0.jpg' in the archive\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"data/raw\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    "    try_gcs=True,\n",
    "    data_dir=data_dir\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset carregado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97fda4",
   "metadata": {},
   "source": [
    "## üîπ 4. Pr√©-processamento das Imagens\n",
    "- Redimensionar imagens para 128x128\n",
    "- Normalizar pixels para valores entre 0 e 1\n",
    "- Criar lotes (batchs) para treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (128, 128) # Redimensionamento das imagens\n",
    "BATCH_SIZE = 32 # Tamanho do batch, ou seja, quantas imagens ser√£o processadas juntas\n",
    "\n",
    "def preprocess(image, label): # Fun√ß√£o de pr√©-processamento\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = image / 255.0  # Normaliza√ß√£o\n",
    "    return image, label\n",
    "\n",
    "ds_train = ds_train.map(preprocess).batch(BATCH_SIZE).shuffle(1000) # Embaralhando os dados de treino\n",
    "ds_test = ds_test.map(preprocess).batch(BATCH_SIZE) # Dados de teste n√£o precisam ser embaralhados\n",
    "\n",
    "print(\"‚úÖ Pr√©-processamento conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aae64ce",
   "metadata": {},
   "source": [
    "## üîπ 5. Visualizar Amostras do Dataset\n",
    "Vamos exibir algumas imagens para verificar se o dataset foi carregado corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27944cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in ds_train.take(1): # Pegando um batch de imagens\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(9):\n",
    "        plt.subplot(3,3,i+1)\n",
    "        plt.imshow(images[i].numpy())\n",
    "        plt.title(\"Cat\" if labels[i].numpy()==0 else \"Dog\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e7886",
   "metadata": {},
   "source": [
    "## üîπ 6. Defini√ß√£o do Modelo CNN\n",
    "A rede neural convolucional ser√° composta por:\n",
    "- Camadas Conv2D + MaxPooling para extrair caracter√≠sticas visuais\n",
    "- Flatten para achatar a matriz em vetor\n",
    "- Dense para classifica√ß√£o bin√°ria (gato ou cachorro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d183e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(*IMG_SIZE, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # Bin√°ria: gato ou cachorro\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae307a",
   "metadata": {},
   "source": [
    "## üîπ 7. Compila√ß√£o e Treinamento do Modelo\n",
    "- Otimizador: Adam (eficiente para CNNs)\n",
    "- Fun√ß√£o de perda: Binary Crossentropy (classifica√ß√£o bin√°ria)\n",
    "- M√©trica: Acur√°cia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( # Compilando o modelo\n",
    "    optimizer='adam', # adam serve para otimiza√ß√£o, ele ajuda a encontrar os melhores pesos para a rede, √© um dos otimizadores mais usados.\n",
    "    loss='binary_crossentropy', # binary_crossentropy √© a fun√ß√£o de perda usada para problemas de classifica√ß√£o bin√°ria.\n",
    "    metrics=['accuracy'] # accuracy √© a m√©trica que mede a propor√ß√£o de previs√µes corretas (acertos) feitas pelo modelo.\n",
    ")\n",
    "\n",
    "history = model.fit( # Treinando o modelo\n",
    "    ds_train,\n",
    "    validation_data=ds_test,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d2466",
   "metadata": {},
   "source": [
    "## üîπ 8. Avalia√ß√£o do Modelo\n",
    "Vamos analisar as curvas de acur√°cia e loss para entender o desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Acur√°cia\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Treino')\n",
    "plt.plot(history.history['val_accuracy'], label='Valida√ß√£o')\n",
    "plt.title('Acur√°cia')\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Treino')\n",
    "plt.plot(history.history['val_loss'], label='Valida√ß√£o')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Treinamento e avalia√ß√£o conclu√≠dos!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
